{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "LSTMtraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshat-Tripathi/AIHack20/blob/master/LSTMtraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLmKthlsuBRZ",
        "colab_type": "text"
      },
      "source": [
        "# Compressor Analytics Dataset\n",
        "\n",
        "Below are a few things to get you started with the Compressor Analytics dataset for AIHack! For more information see [here](https://drive.google.com/file/d/1ojYVHpabfkNz45nQqQLenyoaQbCmWdo4/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH1UeoJNuBRf",
        "colab_type": "code",
        "outputId": "95ffa3ee-8f09-43c6-879d-20d0006377e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Download data - please only run this once\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"Starting download ...\")\n",
        "urls = [\n",
        "    \"https://github.com/aihack20/shell_challenge/releases/download/data/clean_dataset.zip\",\n",
        "    \"https://github.com/aihack20/shell_challenge/releases/download/data/raw_dataset.zip\",\n",
        "]\n",
        "os.makedirs(\"shell_data\", exist_ok=True)\n",
        "for url in urls:\n",
        "    with urllib.request.urlopen(url) as src:\n",
        "        with open(\"tmp.zip\", \"wb\") as dest:\n",
        "            dest.write(src.read())\n",
        "    print(\"Unpacking archive ...\")\n",
        "    with zipfile.ZipFile(\"tmp.zip\") as f:\n",
        "        f.extractall(\"shell_data\")\n",
        "print(\"Done!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting download ...\n",
            "Unpacking archive ...\n",
            "Unpacking archive ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yzRNgXOuBSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqBu_suiuBSY",
        "colab_type": "code",
        "outputId": "6d4a2304-7075-4587-a001-d2a493e4c4bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Load the data (will take a short while)\n",
        "clean_data = pd.read_csv(\"shell_data/clean_dataset.csv\")\n",
        "raw_data = pd.read_csv(\"shell_data/raw_dataset.csv\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K68iQSZ8uBSi",
        "colab_type": "text"
      },
      "source": [
        "## Your Hack ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKt5D-oYuBSm",
        "colab_type": "code",
        "outputId": "71efaa45-5522-42c0-e46b-8dbf57970d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbp2xzA7uBSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(clean_data.dropna())\n",
        "# df = pd.DataFrame(clean_data)\n",
        "heads = list(df)\n",
        "\n",
        "hold = df.to_numpy()\n",
        "df_shape = df.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBkkt8uFv4gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "hold_norm = scaler.fit_transform(hold)\n",
        "\n",
        "ts = 60\n",
        "\n",
        "\n",
        "# nan_arr = np.argwhere(np.isnan(hold))\n",
        "# hold_nom = hold_norm[~np.isnan(hold_norm).any(axis=1)]\n",
        "\n",
        "# nan_arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "983nNP4q504u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A list of time periods (also a list) where is each element is a row of dataframe --> [[index]]\n",
        "values = clean_data.values\n",
        "last_index = 0\n",
        "current_period = [0]\n",
        "periods = []\n",
        "\n",
        "for i in range(1,len(values)):\n",
        "  if last_index == clean_data.iloc[i]['original_index'] - 1:\n",
        "    current_period.append(i)\n",
        "    last_index+=1\n",
        "  else:\n",
        "    periods.append(current_period)\n",
        "    current_period = [i]\n",
        "    last_index = clean_data.iloc[i]['original_index']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAVoDToA6SBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting all data for now, also for digital twin\n",
        "\n",
        "\n",
        "goodc = []\n",
        "\n",
        "for i in periods:\n",
        "  if len(i) > ts:\n",
        "    goodc.append(i[:-ts])\n",
        "\n",
        "\n",
        "from itertools import chain\n",
        "goodcc = list(chain.from_iterable(goodc))\n",
        "\n",
        "\n",
        "x_all = np.empty([len(goodcc), ts, df_shape[1]])\n",
        "y_all = np.empty([len(goodcc), df_shape[1]])\n",
        "\n",
        "\n",
        "for i, j in enumerate(goodcc):\n",
        "  y_all[i] = hold_norm[j+ts]\n",
        "  for k in range(ts):\n",
        "    x_all[i][k] = hold_norm[j+k]\n",
        "\n",
        "train_len = math.ceil(len(y_all)*0.8)\n",
        "x_train = x_all[:train_len]\n",
        "y_train = y_all[:train_len]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDhDWNm0O9Jm",
        "colab_type": "code",
        "outputId": "c1e7cdcb-8799-4354-bf11-bf50874d7a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "solvers = ['adam','SGD','RMSprop','Adagrad','Adadelta','Adamax','Nadam']\n",
        "losses = ['mean_squared_error','mean_absolute_error','mean_absolute_percentage_error',\n",
        "          'mean_squared_logarithmic_error','squared_hinge','hinge','categorical_hinge',\n",
        "          'logcosh','categorical_crossentropy',\n",
        "          'binary_crossentropy', 'kullback_leibler_divergence','poisson','cosine_proximity']\n",
        "          # 'is_categorical_crossentropy']  # ,'huber_loss','sparse_categorical_crossentropy\n",
        "n = 0\n",
        "tries  = [[] for i in range(len(solvers)*len(losses))]\n",
        "# for x in solvers:\n",
        "#   for y in losses:\n",
        "#     tries[n] = [x,y,x+'-'+y]\n",
        "#     n += 1\n",
        "\n",
        "for y in losses:\n",
        "  for x in solvers:\n",
        "    tries[n] = [x,y,x+'-'+y]\n",
        "    n += 1\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['adam', 'mean_squared_error', 'adam-mean_squared_error'], ['SGD', 'mean_squared_error', 'SGD-mean_squared_error'], ['RMSprop', 'mean_squared_error', 'RMSprop-mean_squared_error'], ['Adagrad', 'mean_squared_error', 'Adagrad-mean_squared_error'], ['Adadelta', 'mean_squared_error', 'Adadelta-mean_squared_error'], ['Adamax', 'mean_squared_error', 'Adamax-mean_squared_error'], ['Nadam', 'mean_squared_error', 'Nadam-mean_squared_error'], ['adam', 'mean_absolute_error', 'adam-mean_absolute_error'], ['SGD', 'mean_absolute_error', 'SGD-mean_absolute_error'], ['RMSprop', 'mean_absolute_error', 'RMSprop-mean_absolute_error'], ['Adagrad', 'mean_absolute_error', 'Adagrad-mean_absolute_error'], ['Adadelta', 'mean_absolute_error', 'Adadelta-mean_absolute_error'], ['Adamax', 'mean_absolute_error', 'Adamax-mean_absolute_error'], ['Nadam', 'mean_absolute_error', 'Nadam-mean_absolute_error'], ['adam', 'mean_absolute_percentage_error', 'adam-mean_absolute_percentage_error'], ['SGD', 'mean_absolute_percentage_error', 'SGD-mean_absolute_percentage_error'], ['RMSprop', 'mean_absolute_percentage_error', 'RMSprop-mean_absolute_percentage_error'], ['Adagrad', 'mean_absolute_percentage_error', 'Adagrad-mean_absolute_percentage_error'], ['Adadelta', 'mean_absolute_percentage_error', 'Adadelta-mean_absolute_percentage_error'], ['Adamax', 'mean_absolute_percentage_error', 'Adamax-mean_absolute_percentage_error'], ['Nadam', 'mean_absolute_percentage_error', 'Nadam-mean_absolute_percentage_error'], ['adam', 'mean_squared_logarithmic_error', 'adam-mean_squared_logarithmic_error'], ['SGD', 'mean_squared_logarithmic_error', 'SGD-mean_squared_logarithmic_error'], ['RMSprop', 'mean_squared_logarithmic_error', 'RMSprop-mean_squared_logarithmic_error'], ['Adagrad', 'mean_squared_logarithmic_error', 'Adagrad-mean_squared_logarithmic_error'], ['Adadelta', 'mean_squared_logarithmic_error', 'Adadelta-mean_squared_logarithmic_error'], ['Adamax', 'mean_squared_logarithmic_error', 'Adamax-mean_squared_logarithmic_error'], ['Nadam', 'mean_squared_logarithmic_error', 'Nadam-mean_squared_logarithmic_error'], ['adam', 'squared_hinge', 'adam-squared_hinge'], ['SGD', 'squared_hinge', 'SGD-squared_hinge'], ['RMSprop', 'squared_hinge', 'RMSprop-squared_hinge'], ['Adagrad', 'squared_hinge', 'Adagrad-squared_hinge'], ['Adadelta', 'squared_hinge', 'Adadelta-squared_hinge'], ['Adamax', 'squared_hinge', 'Adamax-squared_hinge'], ['Nadam', 'squared_hinge', 'Nadam-squared_hinge'], ['adam', 'hinge', 'adam-hinge'], ['SGD', 'hinge', 'SGD-hinge'], ['RMSprop', 'hinge', 'RMSprop-hinge'], ['Adagrad', 'hinge', 'Adagrad-hinge'], ['Adadelta', 'hinge', 'Adadelta-hinge'], ['Adamax', 'hinge', 'Adamax-hinge'], ['Nadam', 'hinge', 'Nadam-hinge'], ['adam', 'categorical_hinge', 'adam-categorical_hinge'], ['SGD', 'categorical_hinge', 'SGD-categorical_hinge'], ['RMSprop', 'categorical_hinge', 'RMSprop-categorical_hinge'], ['Adagrad', 'categorical_hinge', 'Adagrad-categorical_hinge'], ['Adadelta', 'categorical_hinge', 'Adadelta-categorical_hinge'], ['Adamax', 'categorical_hinge', 'Adamax-categorical_hinge'], ['Nadam', 'categorical_hinge', 'Nadam-categorical_hinge'], ['adam', 'logcosh', 'adam-logcosh'], ['SGD', 'logcosh', 'SGD-logcosh'], ['RMSprop', 'logcosh', 'RMSprop-logcosh'], ['Adagrad', 'logcosh', 'Adagrad-logcosh'], ['Adadelta', 'logcosh', 'Adadelta-logcosh'], ['Adamax', 'logcosh', 'Adamax-logcosh'], ['Nadam', 'logcosh', 'Nadam-logcosh'], ['adam', 'categorical_crossentropy', 'adam-categorical_crossentropy'], ['SGD', 'categorical_crossentropy', 'SGD-categorical_crossentropy'], ['RMSprop', 'categorical_crossentropy', 'RMSprop-categorical_crossentropy'], ['Adagrad', 'categorical_crossentropy', 'Adagrad-categorical_crossentropy'], ['Adadelta', 'categorical_crossentropy', 'Adadelta-categorical_crossentropy'], ['Adamax', 'categorical_crossentropy', 'Adamax-categorical_crossentropy'], ['Nadam', 'categorical_crossentropy', 'Nadam-categorical_crossentropy'], ['adam', 'binary_crossentropy', 'adam-binary_crossentropy'], ['SGD', 'binary_crossentropy', 'SGD-binary_crossentropy'], ['RMSprop', 'binary_crossentropy', 'RMSprop-binary_crossentropy'], ['Adagrad', 'binary_crossentropy', 'Adagrad-binary_crossentropy'], ['Adadelta', 'binary_crossentropy', 'Adadelta-binary_crossentropy'], ['Adamax', 'binary_crossentropy', 'Adamax-binary_crossentropy'], ['Nadam', 'binary_crossentropy', 'Nadam-binary_crossentropy'], ['adam', 'kullback_leibler_divergence', 'adam-kullback_leibler_divergence'], ['SGD', 'kullback_leibler_divergence', 'SGD-kullback_leibler_divergence'], ['RMSprop', 'kullback_leibler_divergence', 'RMSprop-kullback_leibler_divergence'], ['Adagrad', 'kullback_leibler_divergence', 'Adagrad-kullback_leibler_divergence'], ['Adadelta', 'kullback_leibler_divergence', 'Adadelta-kullback_leibler_divergence'], ['Adamax', 'kullback_leibler_divergence', 'Adamax-kullback_leibler_divergence'], ['Nadam', 'kullback_leibler_divergence', 'Nadam-kullback_leibler_divergence'], ['adam', 'poisson', 'adam-poisson'], ['SGD', 'poisson', 'SGD-poisson'], ['RMSprop', 'poisson', 'RMSprop-poisson'], ['Adagrad', 'poisson', 'Adagrad-poisson'], ['Adadelta', 'poisson', 'Adadelta-poisson'], ['Adamax', 'poisson', 'Adamax-poisson'], ['Nadam', 'poisson', 'Nadam-poisson'], ['adam', 'cosine_proximity', 'adam-cosine_proximity'], ['SGD', 'cosine_proximity', 'SGD-cosine_proximity'], ['RMSprop', 'cosine_proximity', 'RMSprop-cosine_proximity'], ['Adagrad', 'cosine_proximity', 'Adagrad-cosine_proximity'], ['Adadelta', 'cosine_proximity', 'Adadelta-cosine_proximity'], ['Adamax', 'cosine_proximity', 'Adamax-cosine_proximity'], ['Nadam', 'cosine_proximity', 'Nadam-cosine_proximity']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QathGDYp_G0M",
        "colab_type": "code",
        "outputId": "a09d7b32-51a6-4814-9c56-8ecb8950d324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "logtxt = F'/content/gdrive/My Drive/netlog/log.txt'\n",
        "done = []\n",
        "f = open(logtxt,'r')\n",
        "for line in f:\n",
        "  done.append(line)\n",
        "f.close\n",
        "\n",
        "\n",
        "for i,x in enumerate(tries):\n",
        "  if x[2] not in done:\n",
        "\n",
        "    print(str(i+1)+'/'+str(len(tries)))\n",
        "    print(x[2])\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(180, return_sequences=True, input_shape=(ts, df_shape[1])))\n",
        "    model.add(LSTM(180, return_sequences=False))\n",
        "    model.add(Dense(60))\n",
        "    model.add(Dense(df_shape[1]))\n",
        "    model.compile(optimizer=x[0], loss=x[1])\n",
        "\n",
        "    history = model.fit(x_train, y_train, batch_size=1024, epochs=5, verbose=2, shuffle=True)\n",
        "\n",
        "    loss = history.history['loss']\n",
        "\n",
        "\n",
        "    netname = x[2]\n",
        "    fileh5 = F'/content/gdrive/My Drive/nets/' + netname + '.h5'\n",
        "\n",
        "    filetxt = F'/content/gdrive/My Drive/nets/' + netname+ '.txt'\n",
        "\n",
        "    f = open(filetxt, 'w')\n",
        "    # for i,j in enumerate(loss):\n",
        "    #   f.write(str(i) + ' : ' + str(j) + '\\n')\n",
        "    f.write(str(loss))\n",
        "    f.close\n",
        "\n",
        "    f = open(logtxt,'a')\n",
        "    f.write(x[2] + '\\n')\n",
        "    f.close\n",
        "\n",
        "\n",
        "    model.save(fileh5)\n",
        "\n",
        "# drive.flush_and_unmount()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/91\n",
            "adam-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 36s - loss: 0.0655\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0083\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0072\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0056\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0044\n",
            "2/91\n",
            "SGD-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 35s - loss: 0.4780\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.4376\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.3968\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.3447\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.2805\n",
            "3/91\n",
            "RMSprop-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 35s - loss: 0.0438\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0112\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0098\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0085\n",
            "Epoch 5/5\n",
            " - 24s - loss: 0.0073\n",
            "4/91\n",
            "Adagrad-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 36s - loss: 0.0308\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0086\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0084\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0081\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0078\n",
            "5/91\n",
            "Adadelta-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.0550\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0087\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0086\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0085\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0084\n",
            "6/91\n",
            "Adamax-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.0520\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0086\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0082\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0076\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0070\n",
            "7/91\n",
            "Nadam-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.0411\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0098\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0090\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0082\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0074\n",
            "8/91\n",
            "adam-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.1402\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0471\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0456\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0416\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0366\n",
            "9/91\n",
            "SGD-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.6162\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.5820\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.5435\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.4925\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.4297\n",
            "10/91\n",
            "RMSprop-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 39s - loss: 0.1208\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0689\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0642\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0615\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0591\n",
            "11/91\n",
            "Adagrad-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 38s - loss: 0.1125\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0578\n",
            "Epoch 3/5\n",
            " - 24s - loss: 0.0531\n",
            "Epoch 4/5\n",
            " - 24s - loss: 0.0509\n",
            "Epoch 5/5\n",
            " - 24s - loss: 0.0496\n",
            "12/91\n",
            "Adadelta-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.1629\n",
            "Epoch 2/5\n",
            " - 24s - loss: 0.0948\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0868\n",
            "Epoch 4/5\n",
            " - 24s - loss: 0.0818\n",
            "Epoch 5/5\n",
            " - 24s - loss: 0.0782\n",
            "13/91\n",
            "Adamax-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 38s - loss: 0.1136\n",
            "Epoch 2/5\n",
            " - 24s - loss: 0.0470\n",
            "Epoch 3/5\n",
            " - 24s - loss: 0.0462\n",
            "Epoch 4/5\n",
            " - 24s - loss: 0.0449\n",
            "Epoch 5/5\n",
            " - 24s - loss: 0.0427\n",
            "14/91\n",
            "Nadam-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 40s - loss: 0.1166\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0659\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0617\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0597\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0557\n",
            "15/91\n",
            "adam-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 40s - loss: 440819.8689\n",
            "Epoch 2/5\n",
            " - 25s - loss: 78002.5250\n",
            "Epoch 3/5\n",
            " - 25s - loss: 35783.4890\n",
            "Epoch 4/5\n",
            " - 25s - loss: 23202.7861\n",
            "Epoch 5/5\n",
            " - 25s - loss: 19718.6295\n",
            "16/91\n",
            "SGD-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 40s - loss: nan\n",
            "Epoch 2/5\n",
            " - 25s - loss: nan\n",
            "Epoch 3/5\n",
            " - 25s - loss: nan\n",
            "Epoch 4/5\n",
            " - 25s - loss: nan\n",
            "Epoch 5/5\n",
            " - 25s - loss: nan\n",
            "17/91\n",
            "RMSprop-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 40s - loss: 1068066.8271\n",
            "Epoch 2/5\n",
            " - 25s - loss: 175741.5447\n",
            "Epoch 3/5\n",
            " - 25s - loss: 74989.1004\n",
            "Epoch 4/5\n",
            " - 25s - loss: 44857.9609\n",
            "Epoch 5/5\n",
            " - 25s - loss: 33375.9826\n",
            "18/91\n",
            "Adagrad-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 41s - loss: 1625135.2360\n",
            "Epoch 2/5\n",
            " - 25s - loss: 321498.6433\n",
            "Epoch 3/5\n",
            " - 25s - loss: 238346.0875\n",
            "Epoch 4/5\n",
            " - 25s - loss: 159185.9451\n",
            "Epoch 5/5\n",
            " - 25s - loss: 118867.3658\n",
            "19/91\n",
            "Adadelta-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 42s - loss: 799960.7790\n",
            "Epoch 2/5\n",
            " - 25s - loss: 85277.0494\n",
            "Epoch 3/5\n",
            " - 25s - loss: 50637.4142\n",
            "Epoch 4/5\n",
            " - 25s - loss: 43446.2905\n",
            "Epoch 5/5\n",
            " - 25s - loss: 32069.4058\n",
            "20/91\n",
            "Adamax-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 43s - loss: 497530.4511\n",
            "Epoch 2/5\n",
            " - 25s - loss: 93323.2892\n",
            "Epoch 3/5\n",
            " - 25s - loss: 79631.9534\n",
            "Epoch 4/5\n",
            " - 25s - loss: 85215.2438\n",
            "Epoch 5/5\n",
            " - 25s - loss: 55251.5618\n",
            "21/91\n",
            "Nadam-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 43s - loss: 883167.4277\n",
            "Epoch 2/5\n",
            " - 25s - loss: 125655.1544\n",
            "Epoch 3/5\n",
            " - 25s - loss: 64325.0047\n",
            "Epoch 4/5\n",
            " - 25s - loss: 59186.3692\n",
            "Epoch 5/5\n",
            " - 25s - loss: 46873.1492\n",
            "22/91\n",
            "adam-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 43s - loss: 0.0816\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0684\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0645\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0546\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0496\n",
            "23/91\n",
            "SGD-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 43s - loss: 0.2200\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.1919\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.1719\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.1555\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.1428\n",
            "24/91\n",
            "RMSprop-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 44s - loss: 0.0866\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0694\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0637\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0565\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0503\n",
            "25/91\n",
            "Adagrad-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 45s - loss: 0.0695\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0576\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0520\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0492\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0475\n",
            "26/91\n",
            "Adadelta-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 45s - loss: 0.1239\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.1106\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.1066\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.1056\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.1035\n",
            "27/91\n",
            "Adamax-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 45s - loss: 0.0805\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0692\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0646\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0617\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0606\n",
            "28/91\n",
            "Nadam-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 45s - loss: 0.0828\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0700\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0609\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0562\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0521\n",
            "29/91\n",
            "adam-squared_hinge\n",
            "Epoch 1/5\n",
            " - 46s - loss: 0.1623\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0335\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0320\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0315\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0311\n",
            "30/91\n",
            "SGD-squared_hinge\n",
            "Epoch 1/5\n",
            " - 46s - loss: 0.9650\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.9018\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.8199\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.7071\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.5739\n",
            "31/91\n",
            "RMSprop-squared_hinge\n",
            "Epoch 1/5\n",
            " - 46s - loss: 0.1043\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0312\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0294\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0286\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0282\n",
            "32/91\n",
            "Adagrad-squared_hinge\n",
            "Epoch 1/5\n",
            " - 46s - loss: 0.0689\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0332\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0323\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0319\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0316\n",
            "33/91\n",
            "Adadelta-squared_hinge\n",
            "Epoch 1/5\n",
            " - 47s - loss: 0.1612\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0457\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0393\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0363\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0349\n",
            "34/91\n",
            "Adamax-squared_hinge\n",
            "Epoch 1/5\n",
            " - 47s - loss: 0.1267\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0338\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0327\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0322\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0318\n",
            "35/91\n",
            "Nadam-squared_hinge\n",
            "Epoch 1/5\n",
            " - 49s - loss: 0.0944\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0318\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0311\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0307\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0304\n",
            "36/91\n",
            "adam-hinge\n",
            "Epoch 1/5\n",
            " - 49s - loss: 0.1592\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0351\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0330\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0323\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0319\n",
            "37/91\n",
            "SGD-hinge\n",
            "Epoch 1/5\n",
            " - 48s - loss: 0.9849\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.9609\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.9294\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.8850\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.8234\n",
            "38/91\n",
            "RMSprop-hinge\n",
            "Epoch 1/5\n",
            " - 49s - loss: 0.0968\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0317\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0297\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0289\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0284\n",
            "39/91\n",
            "Adagrad-hinge\n",
            "Epoch 1/5\n",
            " - 50s - loss: 0.0695\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0342\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0330\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0324\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0321\n",
            "40/91\n",
            "Adadelta-hinge\n",
            "Epoch 1/5\n",
            " - 50s - loss: 0.1991\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0536\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0455\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0399\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0371\n",
            "41/91\n",
            "Adamax-hinge\n",
            "Epoch 1/5\n",
            " - 50s - loss: 0.1379\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0348\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0336\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0330\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0326\n",
            "42/91\n",
            "Nadam-hinge\n",
            "Epoch 1/5\n",
            " - 51s - loss: 0.1032\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0326\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0316\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0311\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0308\n",
            "43/91\n",
            "adam-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 51s - loss: 0.0183\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "44/91\n",
            "SGD-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 51s - loss: 0.0329\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "45/91\n",
            "RMSprop-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 52s - loss: 0.0158\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "46/91\n",
            "Adagrad-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0152\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0000e+00\n",
            "47/91\n",
            "Adadelta-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0226\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "48/91\n",
            "Adamax-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 53s - loss: 0.0188\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "49/91\n",
            "Nadam-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 53s - loss: 0.0020\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0000e+00\n",
            "50/91\n",
            "adam-logcosh\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0316\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0041\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0035\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0027\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0023\n",
            "51/91\n",
            "SGD-logcosh\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.2196\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.2082\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.1995\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.1914\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.1828\n",
            "52/91\n",
            "RMSprop-logcosh\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0202\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0055\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0049\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0041\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0035\n",
            "53/91\n",
            "Adagrad-logcosh\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0142\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0042\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0041\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0039\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0037\n",
            "54/91\n",
            "Adadelta-logcosh\n",
            "Epoch 1/5\n",
            " - 57s - loss: 0.0354\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0043\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0042\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0042\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0042\n",
            "55/91\n",
            "Adamax-logcosh\n",
            "Epoch 1/5\n",
            " - 56s - loss: 0.0269\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0042\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0039\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0036\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0032\n",
            "56/91\n",
            "Nadam-logcosh\n",
            "Epoch 1/5\n",
            " - 56s - loss: 0.0186\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0050\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0046\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0042\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0036\n",
            "57/91\n",
            "adam-categorical_crossentropy\n",
            "Epoch 1/5\n",
            " - 58s - loss: 2190.3423\n",
            "Epoch 2/5\n",
            " - 26s - loss: 2207.7919\n",
            "Epoch 3/5\n",
            " - 26s - loss: 2175.7374\n",
            "Epoch 4/5\n",
            " - 26s - loss: 2027.2394\n",
            "Epoch 5/5\n",
            " - 26s - loss: 2110.9209\n",
            "58/91\n",
            "SGD-categorical_crossentropy\n",
            "Epoch 1/5\n",
            " - 60s - loss: 3447.7843\n",
            "Epoch 2/5\n",
            " - 26s - loss: 3417.4423\n",
            "Epoch 3/5\n",
            " - 26s - loss: 3387.4362\n",
            "Epoch 4/5\n",
            " - 27s - loss: 3299.4122\n",
            "Epoch 5/5\n",
            " - 26s - loss: 3216.8123\n",
            "59/91\n",
            "RMSprop-categorical_crossentropy\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWo3Wti95Ffy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "ec844255-9f4d-497d-b06c-6ffcb6665639"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "model.add(LSTM(1000, return_sequences=True, input_shape=(60, 363)))\n",
        "model.add(LSTM(1000, return_sequences=False))\n",
        "model.add(Dense(600))\n",
        "model.add(Dense(363))\n",
        "# model.compile(optimizer=x[0], loss=x[1])\n",
        "plot_model(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOUAAAHBCAIAAAAZ4evVAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nk/8PwVqs7IIBGbUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}