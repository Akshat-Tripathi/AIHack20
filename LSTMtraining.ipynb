{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshat-Tripathi/AIHack20/blob/master/LSTMtraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLmKthlsuBRZ",
        "colab_type": "text"
      },
      "source": [
        "# Compressor Analytics Dataset\n",
        "\n",
        "Below are a few things to get you started with the Compressor Analytics dataset for AIHack! For more information see [here](https://drive.google.com/file/d/1ojYVHpabfkNz45nQqQLenyoaQbCmWdo4/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH1UeoJNuBRf",
        "colab_type": "code",
        "outputId": "95ffa3ee-8f09-43c6-879d-20d0006377e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Download data - please only run this once\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"Starting download ...\")\n",
        "urls = [\n",
        "    \"https://github.com/aihack20/shell_challenge/releases/download/data/clean_dataset.zip\",\n",
        "    \"https://github.com/aihack20/shell_challenge/releases/download/data/raw_dataset.zip\",\n",
        "]\n",
        "os.makedirs(\"shell_data\", exist_ok=True)\n",
        "for url in urls:\n",
        "    with urllib.request.urlopen(url) as src:\n",
        "        with open(\"tmp.zip\", \"wb\") as dest:\n",
        "            dest.write(src.read())\n",
        "    print(\"Unpacking archive ...\")\n",
        "    with zipfile.ZipFile(\"tmp.zip\") as f:\n",
        "        f.extractall(\"shell_data\")\n",
        "print(\"Done!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting download ...\n",
            "Unpacking archive ...\n",
            "Unpacking archive ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yzRNgXOuBSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqBu_suiuBSY",
        "colab_type": "code",
        "outputId": "6d4a2304-7075-4587-a001-d2a493e4c4bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Load the data (will take a short while)\n",
        "clean_data = pd.read_csv(\"shell_data/clean_dataset.csv\")\n",
        "raw_data = pd.read_csv(\"shell_data/raw_dataset.csv\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K68iQSZ8uBSi",
        "colab_type": "text"
      },
      "source": [
        "## Your Hack ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKt5D-oYuBSm",
        "colab_type": "code",
        "outputId": "71efaa45-5522-42c0-e46b-8dbf57970d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbp2xzA7uBSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(clean_data.dropna())\n",
        "# df = pd.DataFrame(clean_data)\n",
        "heads = list(df)\n",
        "\n",
        "hold = df.to_numpy()\n",
        "df_shape = df.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBkkt8uFv4gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "hold_norm = scaler.fit_transform(hold)\n",
        "\n",
        "ts = 60\n",
        "\n",
        "\n",
        "# nan_arr = np.argwhere(np.isnan(hold))\n",
        "# hold_nom = hold_norm[~np.isnan(hold_norm).any(axis=1)]\n",
        "\n",
        "# nan_arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "983nNP4q504u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A list of time periods (also a list) where is each element is a row of dataframe --> [[index]]\n",
        "values = clean_data.values\n",
        "last_index = 0\n",
        "current_period = [0]\n",
        "periods = []\n",
        "\n",
        "for i in range(1,len(values)):\n",
        "  if last_index == clean_data.iloc[i]['original_index'] - 1:\n",
        "    current_period.append(i)\n",
        "    last_index+=1\n",
        "  else:\n",
        "    periods.append(current_period)\n",
        "    current_period = [i]\n",
        "    last_index = clean_data.iloc[i]['original_index']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAVoDToA6SBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting all data for now, also for digital twin\n",
        "\n",
        "\n",
        "goodc = []\n",
        "\n",
        "for i in periods:\n",
        "  if len(i) > ts:\n",
        "    goodc.append(i[:-ts])\n",
        "\n",
        "\n",
        "from itertools import chain\n",
        "goodcc = list(chain.from_iterable(goodc))\n",
        "\n",
        "\n",
        "x_all = np.empty([len(goodcc), ts, df_shape[1]])\n",
        "y_all = np.empty([len(goodcc), df_shape[1]])\n",
        "\n",
        "\n",
        "for i, j in enumerate(goodcc):\n",
        "  y_all[i] = hold_norm[j+ts]\n",
        "  for k in range(ts):\n",
        "    x_all[i][k] = hold_norm[j+k]\n",
        "\n",
        "train_len = math.ceil(len(y_all)*0.8)\n",
        "x_train = x_all[:train_len]\n",
        "y_train = y_all[:train_len]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDhDWNm0O9Jm",
        "colab_type": "code",
        "outputId": "c1e7cdcb-8799-4354-bf11-bf50874d7a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "solvers = ['adam','SGD','RMSprop','Adagrad','Adadelta','Adamax','Nadam']\n",
        "losses = ['mean_squared_error','mean_absolute_error','mean_absolute_percentage_error',\n",
        "          'mean_squared_logarithmic_error','squared_hinge','hinge','categorical_hinge',\n",
        "          'logcosh','categorical_crossentropy',\n",
        "          'binary_crossentropy', 'kullback_leibler_divergence','poisson','cosine_proximity']\n",
        "          # 'is_categorical_crossentropy']  # ,'huber_loss','sparse_categorical_crossentropy\n",
        "n = 0\n",
        "tries  = [[] for i in range(len(solvers)*len(losses))]\n",
        "# for x in solvers:\n",
        "#   for y in losses:\n",
        "#     tries[n] = [x,y,x+'-'+y]\n",
        "#     n += 1\n",
        "\n",
        "for y in losses:\n",
        "  for x in solvers:\n",
        "    tries[n] = [x,y,x+'-'+y]\n",
        "    n += 1\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['adam', 'mean_squared_error', 'adam-mean_squared_error'], ['SGD', 'mean_squared_error', 'SGD-mean_squared_error'], ['RMSprop', 'mean_squared_error', 'RMSprop-mean_squared_error'], ['Adagrad', 'mean_squared_error', 'Adagrad-mean_squared_error'], ['Adadelta', 'mean_squared_error', 'Adadelta-mean_squared_error'], ['Adamax', 'mean_squared_error', 'Adamax-mean_squared_error'], ['Nadam', 'mean_squared_error', 'Nadam-mean_squared_error'], ['adam', 'mean_absolute_error', 'adam-mean_absolute_error'], ['SGD', 'mean_absolute_error', 'SGD-mean_absolute_error'], ['RMSprop', 'mean_absolute_error', 'RMSprop-mean_absolute_error'], ['Adagrad', 'mean_absolute_error', 'Adagrad-mean_absolute_error'], ['Adadelta', 'mean_absolute_error', 'Adadelta-mean_absolute_error'], ['Adamax', 'mean_absolute_error', 'Adamax-mean_absolute_error'], ['Nadam', 'mean_absolute_error', 'Nadam-mean_absolute_error'], ['adam', 'mean_absolute_percentage_error', 'adam-mean_absolute_percentage_error'], ['SGD', 'mean_absolute_percentage_error', 'SGD-mean_absolute_percentage_error'], ['RMSprop', 'mean_absolute_percentage_error', 'RMSprop-mean_absolute_percentage_error'], ['Adagrad', 'mean_absolute_percentage_error', 'Adagrad-mean_absolute_percentage_error'], ['Adadelta', 'mean_absolute_percentage_error', 'Adadelta-mean_absolute_percentage_error'], ['Adamax', 'mean_absolute_percentage_error', 'Adamax-mean_absolute_percentage_error'], ['Nadam', 'mean_absolute_percentage_error', 'Nadam-mean_absolute_percentage_error'], ['adam', 'mean_squared_logarithmic_error', 'adam-mean_squared_logarithmic_error'], ['SGD', 'mean_squared_logarithmic_error', 'SGD-mean_squared_logarithmic_error'], ['RMSprop', 'mean_squared_logarithmic_error', 'RMSprop-mean_squared_logarithmic_error'], ['Adagrad', 'mean_squared_logarithmic_error', 'Adagrad-mean_squared_logarithmic_error'], ['Adadelta', 'mean_squared_logarithmic_error', 'Adadelta-mean_squared_logarithmic_error'], ['Adamax', 'mean_squared_logarithmic_error', 'Adamax-mean_squared_logarithmic_error'], ['Nadam', 'mean_squared_logarithmic_error', 'Nadam-mean_squared_logarithmic_error'], ['adam', 'squared_hinge', 'adam-squared_hinge'], ['SGD', 'squared_hinge', 'SGD-squared_hinge'], ['RMSprop', 'squared_hinge', 'RMSprop-squared_hinge'], ['Adagrad', 'squared_hinge', 'Adagrad-squared_hinge'], ['Adadelta', 'squared_hinge', 'Adadelta-squared_hinge'], ['Adamax', 'squared_hinge', 'Adamax-squared_hinge'], ['Nadam', 'squared_hinge', 'Nadam-squared_hinge'], ['adam', 'hinge', 'adam-hinge'], ['SGD', 'hinge', 'SGD-hinge'], ['RMSprop', 'hinge', 'RMSprop-hinge'], ['Adagrad', 'hinge', 'Adagrad-hinge'], ['Adadelta', 'hinge', 'Adadelta-hinge'], ['Adamax', 'hinge', 'Adamax-hinge'], ['Nadam', 'hinge', 'Nadam-hinge'], ['adam', 'categorical_hinge', 'adam-categorical_hinge'], ['SGD', 'categorical_hinge', 'SGD-categorical_hinge'], ['RMSprop', 'categorical_hinge', 'RMSprop-categorical_hinge'], ['Adagrad', 'categorical_hinge', 'Adagrad-categorical_hinge'], ['Adadelta', 'categorical_hinge', 'Adadelta-categorical_hinge'], ['Adamax', 'categorical_hinge', 'Adamax-categorical_hinge'], ['Nadam', 'categorical_hinge', 'Nadam-categorical_hinge'], ['adam', 'logcosh', 'adam-logcosh'], ['SGD', 'logcosh', 'SGD-logcosh'], ['RMSprop', 'logcosh', 'RMSprop-logcosh'], ['Adagrad', 'logcosh', 'Adagrad-logcosh'], ['Adadelta', 'logcosh', 'Adadelta-logcosh'], ['Adamax', 'logcosh', 'Adamax-logcosh'], ['Nadam', 'logcosh', 'Nadam-logcosh'], ['adam', 'categorical_crossentropy', 'adam-categorical_crossentropy'], ['SGD', 'categorical_crossentropy', 'SGD-categorical_crossentropy'], ['RMSprop', 'categorical_crossentropy', 'RMSprop-categorical_crossentropy'], ['Adagrad', 'categorical_crossentropy', 'Adagrad-categorical_crossentropy'], ['Adadelta', 'categorical_crossentropy', 'Adadelta-categorical_crossentropy'], ['Adamax', 'categorical_crossentropy', 'Adamax-categorical_crossentropy'], ['Nadam', 'categorical_crossentropy', 'Nadam-categorical_crossentropy'], ['adam', 'binary_crossentropy', 'adam-binary_crossentropy'], ['SGD', 'binary_crossentropy', 'SGD-binary_crossentropy'], ['RMSprop', 'binary_crossentropy', 'RMSprop-binary_crossentropy'], ['Adagrad', 'binary_crossentropy', 'Adagrad-binary_crossentropy'], ['Adadelta', 'binary_crossentropy', 'Adadelta-binary_crossentropy'], ['Adamax', 'binary_crossentropy', 'Adamax-binary_crossentropy'], ['Nadam', 'binary_crossentropy', 'Nadam-binary_crossentropy'], ['adam', 'kullback_leibler_divergence', 'adam-kullback_leibler_divergence'], ['SGD', 'kullback_leibler_divergence', 'SGD-kullback_leibler_divergence'], ['RMSprop', 'kullback_leibler_divergence', 'RMSprop-kullback_leibler_divergence'], ['Adagrad', 'kullback_leibler_divergence', 'Adagrad-kullback_leibler_divergence'], ['Adadelta', 'kullback_leibler_divergence', 'Adadelta-kullback_leibler_divergence'], ['Adamax', 'kullback_leibler_divergence', 'Adamax-kullback_leibler_divergence'], ['Nadam', 'kullback_leibler_divergence', 'Nadam-kullback_leibler_divergence'], ['adam', 'poisson', 'adam-poisson'], ['SGD', 'poisson', 'SGD-poisson'], ['RMSprop', 'poisson', 'RMSprop-poisson'], ['Adagrad', 'poisson', 'Adagrad-poisson'], ['Adadelta', 'poisson', 'Adadelta-poisson'], ['Adamax', 'poisson', 'Adamax-poisson'], ['Nadam', 'poisson', 'Nadam-poisson'], ['adam', 'cosine_proximity', 'adam-cosine_proximity'], ['SGD', 'cosine_proximity', 'SGD-cosine_proximity'], ['RMSprop', 'cosine_proximity', 'RMSprop-cosine_proximity'], ['Adagrad', 'cosine_proximity', 'Adagrad-cosine_proximity'], ['Adadelta', 'cosine_proximity', 'Adadelta-cosine_proximity'], ['Adamax', 'cosine_proximity', 'Adamax-cosine_proximity'], ['Nadam', 'cosine_proximity', 'Nadam-cosine_proximity']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QathGDYp_G0M",
        "colab_type": "code",
        "outputId": "a09d7b32-51a6-4814-9c56-8ecb8950d324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import LSTM, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "logtxt = F'/content/gdrive/My Drive/netlog/log.txt'\n",
        "done = []\n",
        "f = open(logtxt,'r')\n",
        "for line in f:\n",
        "  done.append(line)\n",
        "f.close\n",
        "\n",
        "\n",
        "for i,x in enumerate(tries):\n",
        "  if x[2] not in done:\n",
        "\n",
        "    print(str(i+1)+'/'+str(len(tries)))\n",
        "    print(x[2])\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(180, return_sequences=True, input_shape=(ts, df_shape[1])))\n",
        "    model.add(LSTM(180, return_sequences=False))\n",
        "    model.add(Dense(60))\n",
        "    model.add(Dense(df_shape[1]))\n",
        "    model.compile(optimizer=x[0], loss=x[1])\n",
        "\n",
        "    history = model.fit(x_train, y_train, batch_size=1024, epochs=5, verbose=2, shuffle=True)\n",
        "\n",
        "    loss = history.history['loss']\n",
        "\n",
        "\n",
        "    netname = x[2]\n",
        "    fileh5 = F'/content/gdrive/My Drive/nets/' + netname + '.h5'\n",
        "\n",
        "    filetxt = F'/content/gdrive/My Drive/nets/' + netname+ '.txt'\n",
        "\n",
        "    f = open(filetxt, 'w')\n",
        "    # for i,j in enumerate(loss):\n",
        "    #   f.write(str(i) + ' : ' + str(j) + '\\n')\n",
        "    f.write(str(loss))\n",
        "    f.close\n",
        "\n",
        "    f = open(logtxt,'a')\n",
        "    f.write(x[2] + '\\n')\n",
        "    f.close\n",
        "\n",
        "\n",
        "    model.save(fileh5)\n",
        "\n",
        "# drive.flush_and_unmount()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/91\n",
            "adam-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 36s - loss: 0.0655\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0083\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0072\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0056\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0044\n",
            "2/91\n",
            "SGD-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 35s - loss: 0.4780\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.4376\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.3968\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.3447\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.2805\n",
            "3/91\n",
            "RMSprop-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 35s - loss: 0.0438\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0112\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0098\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0085\n",
            "Epoch 5/5\n",
            " - 24s - loss: 0.0073\n",
            "4/91\n",
            "Adagrad-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 36s - loss: 0.0308\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0086\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0084\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0081\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0078\n",
            "5/91\n",
            "Adadelta-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.0550\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0087\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0086\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0085\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0084\n",
            "6/91\n",
            "Adamax-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.0520\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0086\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0082\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0076\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0070\n",
            "7/91\n",
            "Nadam-mean_squared_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.0411\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0098\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0090\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0082\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0074\n",
            "8/91\n",
            "adam-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.1402\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0471\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0456\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0416\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0366\n",
            "9/91\n",
            "SGD-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.6162\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.5820\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.5435\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.4925\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.4297\n",
            "10/91\n",
            "RMSprop-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 39s - loss: 0.1208\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0689\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0642\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0615\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0591\n",
            "11/91\n",
            "Adagrad-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 38s - loss: 0.1125\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0578\n",
            "Epoch 3/5\n",
            " - 24s - loss: 0.0531\n",
            "Epoch 4/5\n",
            " - 24s - loss: 0.0509\n",
            "Epoch 5/5\n",
            " - 24s - loss: 0.0496\n",
            "12/91\n",
            "Adadelta-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 37s - loss: 0.1629\n",
            "Epoch 2/5\n",
            " - 24s - loss: 0.0948\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0868\n",
            "Epoch 4/5\n",
            " - 24s - loss: 0.0818\n",
            "Epoch 5/5\n",
            " - 24s - loss: 0.0782\n",
            "13/91\n",
            "Adamax-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 38s - loss: 0.1136\n",
            "Epoch 2/5\n",
            " - 24s - loss: 0.0470\n",
            "Epoch 3/5\n",
            " - 24s - loss: 0.0462\n",
            "Epoch 4/5\n",
            " - 24s - loss: 0.0449\n",
            "Epoch 5/5\n",
            " - 24s - loss: 0.0427\n",
            "14/91\n",
            "Nadam-mean_absolute_error\n",
            "Epoch 1/5\n",
            " - 40s - loss: 0.1166\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0659\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0617\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0597\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0557\n",
            "15/91\n",
            "adam-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 40s - loss: 440819.8689\n",
            "Epoch 2/5\n",
            " - 25s - loss: 78002.5250\n",
            "Epoch 3/5\n",
            " - 25s - loss: 35783.4890\n",
            "Epoch 4/5\n",
            " - 25s - loss: 23202.7861\n",
            "Epoch 5/5\n",
            " - 25s - loss: 19718.6295\n",
            "16/91\n",
            "SGD-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 40s - loss: nan\n",
            "Epoch 2/5\n",
            " - 25s - loss: nan\n",
            "Epoch 3/5\n",
            " - 25s - loss: nan\n",
            "Epoch 4/5\n",
            " - 25s - loss: nan\n",
            "Epoch 5/5\n",
            " - 25s - loss: nan\n",
            "17/91\n",
            "RMSprop-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 40s - loss: 1068066.8271\n",
            "Epoch 2/5\n",
            " - 25s - loss: 175741.5447\n",
            "Epoch 3/5\n",
            " - 25s - loss: 74989.1004\n",
            "Epoch 4/5\n",
            " - 25s - loss: 44857.9609\n",
            "Epoch 5/5\n",
            " - 25s - loss: 33375.9826\n",
            "18/91\n",
            "Adagrad-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 41s - loss: 1625135.2360\n",
            "Epoch 2/5\n",
            " - 25s - loss: 321498.6433\n",
            "Epoch 3/5\n",
            " - 25s - loss: 238346.0875\n",
            "Epoch 4/5\n",
            " - 25s - loss: 159185.9451\n",
            "Epoch 5/5\n",
            " - 25s - loss: 118867.3658\n",
            "19/91\n",
            "Adadelta-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 42s - loss: 799960.7790\n",
            "Epoch 2/5\n",
            " - 25s - loss: 85277.0494\n",
            "Epoch 3/5\n",
            " - 25s - loss: 50637.4142\n",
            "Epoch 4/5\n",
            " - 25s - loss: 43446.2905\n",
            "Epoch 5/5\n",
            " - 25s - loss: 32069.4058\n",
            "20/91\n",
            "Adamax-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 43s - loss: 497530.4511\n",
            "Epoch 2/5\n",
            " - 25s - loss: 93323.2892\n",
            "Epoch 3/5\n",
            " - 25s - loss: 79631.9534\n",
            "Epoch 4/5\n",
            " - 25s - loss: 85215.2438\n",
            "Epoch 5/5\n",
            " - 25s - loss: 55251.5618\n",
            "21/91\n",
            "Nadam-mean_absolute_percentage_error\n",
            "Epoch 1/5\n",
            " - 43s - loss: 883167.4277\n",
            "Epoch 2/5\n",
            " - 25s - loss: 125655.1544\n",
            "Epoch 3/5\n",
            " - 25s - loss: 64325.0047\n",
            "Epoch 4/5\n",
            " - 25s - loss: 59186.3692\n",
            "Epoch 5/5\n",
            " - 25s - loss: 46873.1492\n",
            "22/91\n",
            "adam-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 43s - loss: 0.0816\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0684\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0645\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0546\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0496\n",
            "23/91\n",
            "SGD-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 43s - loss: 0.2200\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.1919\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.1719\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.1555\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.1428\n",
            "24/91\n",
            "RMSprop-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 44s - loss: 0.0866\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0694\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0637\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0565\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0503\n",
            "25/91\n",
            "Adagrad-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 45s - loss: 0.0695\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0576\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0520\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0492\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0475\n",
            "26/91\n",
            "Adadelta-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 45s - loss: 0.1239\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.1106\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.1066\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.1056\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.1035\n",
            "27/91\n",
            "Adamax-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 45s - loss: 0.0805\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0692\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0646\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0617\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0606\n",
            "28/91\n",
            "Nadam-mean_squared_logarithmic_error\n",
            "Epoch 1/5\n",
            " - 45s - loss: 0.0828\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0700\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0609\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0562\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0521\n",
            "29/91\n",
            "adam-squared_hinge\n",
            "Epoch 1/5\n",
            " - 46s - loss: 0.1623\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0335\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0320\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0315\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0311\n",
            "30/91\n",
            "SGD-squared_hinge\n",
            "Epoch 1/5\n",
            " - 46s - loss: 0.9650\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.9018\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.8199\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.7071\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.5739\n",
            "31/91\n",
            "RMSprop-squared_hinge\n",
            "Epoch 1/5\n",
            " - 46s - loss: 0.1043\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0312\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0294\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0286\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0282\n",
            "32/91\n",
            "Adagrad-squared_hinge\n",
            "Epoch 1/5\n",
            " - 46s - loss: 0.0689\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0332\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0323\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0319\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0316\n",
            "33/91\n",
            "Adadelta-squared_hinge\n",
            "Epoch 1/5\n",
            " - 47s - loss: 0.1612\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0457\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0393\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0363\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0349\n",
            "34/91\n",
            "Adamax-squared_hinge\n",
            "Epoch 1/5\n",
            " - 47s - loss: 0.1267\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0338\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0327\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0322\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0318\n",
            "35/91\n",
            "Nadam-squared_hinge\n",
            "Epoch 1/5\n",
            " - 49s - loss: 0.0944\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0318\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0311\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0307\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0304\n",
            "36/91\n",
            "adam-hinge\n",
            "Epoch 1/5\n",
            " - 49s - loss: 0.1592\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0351\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0330\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0323\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0319\n",
            "37/91\n",
            "SGD-hinge\n",
            "Epoch 1/5\n",
            " - 48s - loss: 0.9849\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.9609\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.9294\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.8850\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.8234\n",
            "38/91\n",
            "RMSprop-hinge\n",
            "Epoch 1/5\n",
            " - 49s - loss: 0.0968\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0317\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0297\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0289\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0284\n",
            "39/91\n",
            "Adagrad-hinge\n",
            "Epoch 1/5\n",
            " - 50s - loss: 0.0695\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0342\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0330\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0324\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0321\n",
            "40/91\n",
            "Adadelta-hinge\n",
            "Epoch 1/5\n",
            " - 50s - loss: 0.1991\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0536\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0455\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0399\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0371\n",
            "41/91\n",
            "Adamax-hinge\n",
            "Epoch 1/5\n",
            " - 50s - loss: 0.1379\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0348\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0336\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0330\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0326\n",
            "42/91\n",
            "Nadam-hinge\n",
            "Epoch 1/5\n",
            " - 51s - loss: 0.1032\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0326\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0316\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0311\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0308\n",
            "43/91\n",
            "adam-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 51s - loss: 0.0183\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "44/91\n",
            "SGD-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 51s - loss: 0.0329\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "45/91\n",
            "RMSprop-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 52s - loss: 0.0158\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "46/91\n",
            "Adagrad-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0152\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0000e+00\n",
            "47/91\n",
            "Adadelta-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0226\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "48/91\n",
            "Adamax-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 53s - loss: 0.0188\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "49/91\n",
            "Nadam-categorical_hinge\n",
            "Epoch 1/5\n",
            " - 53s - loss: 0.0020\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0000e+00\n",
            "50/91\n",
            "adam-logcosh\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0316\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0041\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0035\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0027\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0023\n",
            "51/91\n",
            "SGD-logcosh\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.2196\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.2082\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.1995\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.1914\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.1828\n",
            "52/91\n",
            "RMSprop-logcosh\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0202\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0055\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0049\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0041\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0035\n",
            "53/91\n",
            "Adagrad-logcosh\n",
            "Epoch 1/5\n",
            " - 54s - loss: 0.0142\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0042\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0041\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0039\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0037\n",
            "54/91\n",
            "Adadelta-logcosh\n",
            "Epoch 1/5\n",
            " - 57s - loss: 0.0354\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0043\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0042\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0042\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0042\n",
            "55/91\n",
            "Adamax-logcosh\n",
            "Epoch 1/5\n",
            " - 56s - loss: 0.0269\n",
            "Epoch 2/5\n",
            " - 26s - loss: 0.0042\n",
            "Epoch 3/5\n",
            " - 26s - loss: 0.0039\n",
            "Epoch 4/5\n",
            " - 26s - loss: 0.0036\n",
            "Epoch 5/5\n",
            " - 26s - loss: 0.0032\n",
            "56/91\n",
            "Nadam-logcosh\n",
            "Epoch 1/5\n",
            " - 56s - loss: 0.0186\n",
            "Epoch 2/5\n",
            " - 25s - loss: 0.0050\n",
            "Epoch 3/5\n",
            " - 25s - loss: 0.0046\n",
            "Epoch 4/5\n",
            " - 25s - loss: 0.0042\n",
            "Epoch 5/5\n",
            " - 25s - loss: 0.0036\n",
            "57/91\n",
            "adam-categorical_crossentropy\n",
            "Epoch 1/5\n",
            " - 58s - loss: 2190.3423\n",
            "Epoch 2/5\n",
            " - 26s - loss: 2207.7919\n",
            "Epoch 3/5\n",
            " - 26s - loss: 2175.7374\n",
            "Epoch 4/5\n",
            " - 26s - loss: 2027.2394\n",
            "Epoch 5/5\n",
            " - 26s - loss: 2110.9209\n",
            "58/91\n",
            "SGD-categorical_crossentropy\n",
            "Epoch 1/5\n",
            " - 60s - loss: 3447.7843\n",
            "Epoch 2/5\n",
            " - 26s - loss: 3417.4423\n",
            "Epoch 3/5\n",
            " - 26s - loss: 3387.4362\n",
            "Epoch 4/5\n",
            " - 27s - loss: 3299.4122\n",
            "Epoch 5/5\n",
            " - 26s - loss: 3216.8123\n",
            "59/91\n",
            "RMSprop-categorical_crossentropy\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWo3Wti95Ffy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}